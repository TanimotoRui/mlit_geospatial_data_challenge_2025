"""
åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

æ–°è¦è¿½åŠ ç‰¹å¾´é‡:
1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·¯åº¦çµŒåº¦ï¼‰+ é›†ç´„ç‰¹å¾´é‡
2. Target Encodingï¼ˆåœ°åŸŸåˆ¥å¹³å‡ä¾¡æ ¼ãªã©ï¼‰
3. è·é›¢ç‰¹å¾´é‡ï¼ˆä¸»è¦éƒ½å¸‚ã¾ã§ã®è·é›¢ï¼‰
4. æ´¾ç”Ÿç‰¹å¾´é‡ï¼ˆç¯‰å¹´æ•°ã€å˜ä¾¡ãªã©ï¼‰
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
import numpy as np
from datetime import datetime
import warnings

from src.data.preprocess import preprocess_for_catboost
from src.features.geo_features import (
    create_kmeans_clusters,
    create_cluster_aggregation_features,
    create_target_encoding_features,
    create_distance_features,
    create_derived_features
)
from src.models.train_catboost import train_catboost_cv, predict_with_models

warnings.filterwarnings('ignore')

# ãƒ‘ã‚¹è¨­å®š
TRAIN_PATH = "data/raw/train.csv"
TEST_PATH = "data/raw/test.csv"
SAMPLE_PATH = "data/raw/sample_submit.csv"
OUTPUT_DIR = "submissions/exp003_geo_features"

print("=" * 60)
print("åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³")
print("=" * 60)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
print("\n[1] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
train = pd.read_csv(TRAIN_PATH)
test = pd.read_csv(TEST_PATH)
sample_sub = pd.read_csv(SAMPLE_PATH, header=None, names=['id', 'money_room'])

print(f"Train shape: {train.shape}")
print(f"Test shape: {test.shape}")

# åŸºæœ¬å‰å‡¦ç†
print("\n[2] åŸºæœ¬å‰å‡¦ç†...")
train_features, test_features, target, cat_features = preprocess_for_catboost(
    train, test, target_col='money_room', apply_log=True
)

# åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®è¿½åŠ 
print("\n" + "=" * 60)
print("åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®ä½œæˆ")
print("=" * 60)

# ç›®çš„å¤‰æ•°ã‚’ä¸€æ™‚çš„ã«çµåˆï¼ˆTarget Encodingç”¨ï¼‰
train_with_target = train_features.copy()
train_with_target['money_room'] = target

# 1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
train_with_target, test_features, kmeans = create_kmeans_clusters(
    train_with_target, test_features,
    lat_col='lat', lon_col='lon',
    n_clusters=50, random_state=42
)

# 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã®é›†ç´„ç‰¹å¾´é‡
train_with_target, test_features = create_cluster_aggregation_features(
    train_with_target, test_features,
    target_col='money_room',
    cluster_col='geo_cluster',
    agg_cols=['house_area', 'year_built', 'walk_distance1', 'money_kyoueki']
)

# 3. Target Encoding
train_with_target, test_features = create_target_encoding_features(
    train_with_target, test_features,
    target_col='money_room',
    categorical_cols=['city', 'prefecture', 'eki_name1'],
    smoothing=10.0
)

# 4. è·é›¢ç‰¹å¾´é‡
train_with_target = create_distance_features(train_with_target, lat_col='lat', lon_col='lon')
test_features = create_distance_features(test_features, lat_col='lat', lon_col='lon')

# 5. æ´¾ç”Ÿç‰¹å¾´é‡
train_with_target = create_derived_features(train_with_target)
test_features = create_derived_features(test_features)

# ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
target = train_with_target['money_room']
train_features = train_with_target.drop(columns=['money_room'])

print(f"\næœ€çµ‚çš„ãªç‰¹å¾´é‡æ•°: {len(train_features.columns)}")

# ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®æ›´æ–°ï¼ˆæ–°ã—ãè¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã¯æ•°å€¤å‹ï¼‰
# geo_clusterã¯ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã¨ã—ã¦æ‰±ã†
if 'geo_cluster' in train_features.columns:
    train_features['geo_cluster'] = train_features['geo_cluster'].astype(str)
    test_features['geo_cluster'] = test_features['geo_cluster'].astype(str)
    if 'geo_cluster' not in cat_features:
        cat_features.append('geo_cluster')

print(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡æ•°: {len(cat_features)}")

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
params = {
    'iterations': 1000,
    'learning_rate': 0.05,
    'depth': 6,
    'loss_function': 'MAE',
    'eval_metric': 'MAE',
    'random_seed': 42,
    'verbose': 100,
    'early_stopping_rounds': 50,
}

# Cross Validation
print("\n" + "=" * 60)
print("Cross Validation")
print("=" * 60)
models, cv_scores = train_catboost_cv(
    train_features, target, cat_features,
    n_splits=5, params=params, verbose=100
)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
print("\n[3] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬...")
predictions = predict_with_models(
    models, test_features, cat_features, apply_expm1=True
)

# Submissionä½œæˆ
print("\n[4] Submissionä½œæˆ...")
submission = sample_sub.copy()
submission['money_room'] = predictions.astype(int)

# ä¿å­˜
os.makedirs(OUTPUT_DIR, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = f"{OUTPUT_DIR}/submission_{timestamp}.csv"
submission.to_csv(output_path, index=False, header=False)

print(f"\n{'='*60}")
print(f"å®Œäº†!")
print(f"{'='*60}")
print(f"Submission saved to: {output_path}")
print(f"\näºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
print(submission['money_room'].describe())
print(f"\nCV MAPE: {np.mean(cv_scores):.4f}% (+/- {np.std(cv_scores):.4f}%)")
print(f"{'='*60}")
print("Ready to submit! ğŸš€")
print(f"{'='*60}")

# ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜
print("\n[5] ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜...")
feature_importance = pd.DataFrame({
    'feature': train_features.columns,
    'importance': models[0].feature_importances_
}).sort_values('importance', ascending=False)

importance_path = f"{OUTPUT_DIR}/feature_importance_{timestamp}.csv"
feature_importance.to_csv(importance_path, index=False)
print(f"Feature importance saved to: {importance_path}")

print("\nTop 30 é‡è¦ãªç‰¹å¾´é‡:")
print(feature_importance.head(30).to_string(index=False))

