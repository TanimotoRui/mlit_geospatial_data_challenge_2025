"""
åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

æ–°è¦è¿½åŠ ç‰¹å¾´é‡:
1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·¯åº¦çµŒåº¦ï¼‰+ é›†ç´„ç‰¹å¾´é‡
2. Target Encodingï¼ˆåœ°åŸŸåˆ¥å¹³å‡ä¾¡æ ¼ãªã©ï¼‰
3. è·é›¢ç‰¹å¾´é‡ï¼ˆä¸»è¦éƒ½å¸‚ã¾ã§ã®è·é›¢ï¼‰
4. æ´¾ç”Ÿç‰¹å¾´é‡ï¼ˆç¯‰å¹´æ•°ã€å˜ä¾¡ãªã©ï¼‰
"""

import gc
import pickle
import sys
import time
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import warnings  # noqa: E402

import numpy as np  # noqa: E402
import pandas as pd  # noqa: E402
from tqdm import tqdm  # noqa: E402

from src.data.preprocess import preprocess_for_catboost  # noqa: E402
from src.features.geo_features import (  # noqa: E402
    create_cluster_aggregation_features, create_derived_features,
    create_distance_features, create_kmeans_clusters,
    create_target_encoding_features)
from src.models.train_catboost import (predict_with_models,  # noqa: E402
                                       train_catboost_cv)

# warnings.filterwarnings("ignore")  # Warningè¡¨ç¤ºã‚’æœ‰åŠ¹åŒ–

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = project_root / "data"
TRAIN_PATH = DATA_DIR / "raw" / "train.csv"
TEST_PATH = DATA_DIR / "raw" / "test.csv"
SAMPLE_PATH = DATA_DIR / "raw" / "sample_submit.csv"
OUTPUT_DIR = project_root / "submissions" / "exp003_geo_features"
PROCESSED_DIR = DATA_DIR / "processed"

# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
PROCESSED_TRAIN = PROCESSED_DIR / "train_processed.parquet"
PROCESSED_TEST = PROCESSED_DIR / "test_processed.parquet"
PROCESSED_TARGET = PROCESSED_DIR / "target.parquet"
PROCESSED_CAT_FEATURES = PROCESSED_DIR / "cat_features.pkl"

print("=" * 80)
print("ğŸš€ åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³")
print("=" * 80)
print(f"é–‹å§‹æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("=" * 80)

# å…¨ä½“ã®å‡¦ç†æ™‚é–“ã‚’è¨ˆæ¸¬
overall_start_time = time.time()

# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨ãƒ­ãƒ¼ãƒ‰
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
use_cache = all(
    [
        PROCESSED_TRAIN.exists(),
        PROCESSED_TEST.exists(),
        PROCESSED_TARGET.exists(),
        PROCESSED_CAT_FEATURES.exists(),
    ]
)

if use_cache:
    print("\nâœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
    load_start = time.time()

    train_features = pd.read_parquet(PROCESSED_TRAIN)
    print(f"  ğŸ“ train_processed.parquet èª­ã¿è¾¼ã¿å®Œäº†")

    test_features = pd.read_parquet(PROCESSED_TEST)
    print(f"  ğŸ“ test_processed.parquet èª­ã¿è¾¼ã¿å®Œäº†")

    target = pd.read_parquet(PROCESSED_TARGET).squeeze()
    print(f"  ğŸ“ target.parquet èª­ã¿è¾¼ã¿å®Œäº†")

    with open(PROCESSED_CAT_FEATURES, "rb") as f:
        cat_features = pickle.load(f)
    print(f"  ğŸ“ cat_features.pkl èª­ã¿è¾¼ã¿å®Œäº†")

    load_time = time.time() - load_start
    print(f"\n  â±ï¸  ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ™‚é–“: {load_time:.2f}ç§’")
    print(f"\n  ğŸ“Š Train shape: {train_features.shape}")
    print(f"  ğŸ“Š Test shape: {test_features.shape}")
    print(f"  ğŸ“Š ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡æ•°: {len(cat_features)}")

else:
    print("\nğŸ”„ å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")
    preprocess_start = time.time()

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n" + "=" * 80)
    print("[STEP 1/7] ğŸ“‚ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿")
    print("=" * 80)
    step_start = time.time()

    train = pd.read_csv(TRAIN_PATH, low_memory=False)
    print(f"  âœ“ Train data loaded: {train.shape}")

    test = pd.read_csv(TEST_PATH, low_memory=False)
    print(f"  âœ“ Test data loaded: {test.shape}")

    print(f"  â±ï¸  èª­ã¿è¾¼ã¿æ™‚é–“: {time.time() - step_start:.2f}ç§’")

    # åŸºæœ¬å‰å‡¦ç†
    print("\n" + "=" * 80)
    print("[STEP 2/7] ğŸ”§ åŸºæœ¬å‰å‡¦ç†")
    print("=" * 80)
    step_start = time.time()

    train_features, test_features, target, cat_features = preprocess_for_catboost(
        train, test, target_col="money_room", apply_log=True
    )

    print(f"  â±ï¸  å‰å‡¦ç†æ™‚é–“: {time.time() - step_start:.2f}ç§’")

    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤
    del train, test
    gc.collect()

    # åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®è¿½åŠ 
    print("\n" + "=" * 80)
    print("[STEP 3/7] ğŸŒ åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®ä½œæˆ")
    print("=" * 80)
    geo_start = time.time()

    # ç›®çš„å¤‰æ•°ã‚’ä¸€æ™‚çš„ã«çµåˆï¼ˆTarget Encodingç”¨ï¼‰
    train_with_target = train_features.copy()
    train_with_target["money_room"] = target

    # train_featuresã¯ä¸€æ—¦ä¸è¦
    del train_features
    gc.collect()

    # 1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    print("\n  [3-1] K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°...")
    substep_start = time.time()
    train_with_target, test_features, kmeans = create_kmeans_clusters(
        train_with_target,
        test_features,
        lat_col="lat",
        lon_col="lon",
        n_clusters=50,
        random_state=42,
    )
    print(f"        â±ï¸  {time.time() - substep_start:.2f}ç§’")

    # kmeansãƒ¢ãƒ‡ãƒ«ã¯ä¸è¦
    del kmeans
    gc.collect()

    # 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã®é›†ç´„ç‰¹å¾´é‡
    print("\n  [3-2] ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é›†ç´„ç‰¹å¾´é‡...")
    substep_start = time.time()
    train_with_target, test_features = create_cluster_aggregation_features(
        train_with_target,
        test_features,
        target_col="money_room",
        cluster_col="geo_cluster",
        agg_cols=["house_area", "year_built", "walk_distance1", "money_kyoueki"],
    )
    print(f"        â±ï¸  {time.time() - substep_start:.2f}ç§’")

    # 3. Target Encoding
    print("\n  [3-3] Target Encoding...")
    substep_start = time.time()
    train_with_target, test_features = create_target_encoding_features(
        train_with_target,
        test_features,
        target_col="money_room",
        categorical_cols=["city", "prefecture", "eki_name1"],
        smoothing=10.0,
    )
    print(f"        â±ï¸  {time.time() - substep_start:.2f}ç§’")

    # 4. è·é›¢ç‰¹å¾´é‡
    print("\n  [3-4] è·é›¢ç‰¹å¾´é‡...")
    substep_start = time.time()
    train_with_target = create_distance_features(
        train_with_target, lat_col="lat", lon_col="lon"
    )
    test_features = create_distance_features(
        test_features, lat_col="lat", lon_col="lon"
    )
    print(f"        â±ï¸  {time.time() - substep_start:.2f}ç§’")

    # 5. æ´¾ç”Ÿç‰¹å¾´é‡
    print("\n  [3-5] æ´¾ç”Ÿç‰¹å¾´é‡...")
    substep_start = time.time()
    train_with_target = create_derived_features(train_with_target)
    test_features = create_derived_features(test_features)
    print(f"        â±ï¸  {time.time() - substep_start:.2f}ç§’")

    print(f"\n  ğŸŒ åœ°ç†ç©ºé–“ç‰¹å¾´é‡ä½œæˆ å®Œäº†: {time.time() - geo_start:.2f}ç§’")

    # ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
    target = train_with_target["money_room"]
    train_features = train_with_target.drop(columns=["money_room"])

    # train_with_targetã¯ä¸è¦
    del train_with_target
    gc.collect()

    print(f"\n  ğŸ“Š æœ€çµ‚çš„ãªç‰¹å¾´é‡æ•°: {len(train_features.columns)}")

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®æ›´æ–°ï¼ˆæ–°ã—ãè¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã¯æ•°å€¤å‹ï¼‰
    # geo_clusterã¯ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã¨ã—ã¦æ‰±ã†
    if "geo_cluster" in train_features.columns:
        train_features["geo_cluster"] = train_features["geo_cluster"].astype(str)
        test_features["geo_cluster"] = test_features["geo_cluster"].astype(str)
        if "geo_cluster" not in cat_features:
            cat_features.append("geo_cluster")

    print(f"  ğŸ“Š ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡æ•°: {len(cat_features)}")

    # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    print("\n" + "=" * 80)
    print("[STEP 4/7] ğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜")
    print("=" * 80)
    save_start = time.time()

    train_features.to_parquet(PROCESSED_TRAIN, index=False)
    print(f"  âœ“ train_processed.parquet ä¿å­˜å®Œäº†")

    test_features.to_parquet(PROCESSED_TEST, index=False)
    print(f"  âœ“ test_processed.parquet ä¿å­˜å®Œäº†")

    pd.DataFrame({"target": target}).to_parquet(PROCESSED_TARGET, index=False)
    print(f"  âœ“ target.parquet ä¿å­˜å®Œäº†")

    with open(PROCESSED_CAT_FEATURES, "wb") as f:
        pickle.dump(cat_features, f)
    print(f"  âœ“ cat_features.pkl ä¿å­˜å®Œäº†")

    print(f"  â±ï¸  ä¿å­˜æ™‚é–“: {time.time() - save_start:.2f}ç§’")
    print(f"  ğŸ“ ä¿å­˜å…ˆ: {PROCESSED_DIR}/")

    preprocess_time = time.time() - preprocess_start
    print(f"\n  âœ… å‰å‡¦ç† å®Œäº†: {preprocess_time:.2f}ç§’ ({preprocess_time/60:.1f}åˆ†)")

# sample_submitã¯å¸¸ã«èª­ã¿è¾¼ã‚€ï¼ˆè»½ã„ã®ã§ï¼‰
sample_sub = pd.read_csv(SAMPLE_PATH, header=None, names=["id", "money_room"])

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
params = {
    "iterations": 500,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    "learning_rate": 0.05,
    "depth": 5,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    "loss_function": "MAE",
    "eval_metric": "MAE",
    "random_seed": 42,
    "verbose": 100,
    "early_stopping_rounds": 50,
}

# Cross Validation
print("\n" + "=" * 80)
print("[STEP 5/7] ğŸ¤– Cross Validation (3-Fold)")
print("=" * 80)
print(f"  ğŸ“Š Train samples: {len(train_features):,}")
print(f"  ğŸ“Š Features: {len(train_features.columns)}")
print(f"  ğŸ“Š Categorical features: {len(cat_features)}")
print(f"  ğŸ¯ Model: CatBoost Regressor")
print(
    f"  ğŸ”§ Iterations: {params['iterations']}, Depth: {params['depth']}, LR: {params['learning_rate']}"
)
print("=" * 80)

cv_start = time.time()
models, cv_scores = train_catboost_cv(
    train_features,
    target,
    cat_features,
    n_splits=3,
    params=params,
    verbose=100,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
)
cv_time = time.time() - cv_start
print(f"\n  â±ï¸  CVæ™‚é–“: {cv_time:.2f}ç§’ ({cv_time/60:.1f}åˆ†)")

# targetã¯ã‚‚ã†ä¸è¦
del target
gc.collect()

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
print("\n" + "=" * 80)
print("[STEP 6/7] ğŸ”® ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬")
print("=" * 80)
pred_start = time.time()

predictions = predict_with_models(models, test_features, cat_features, apply_expm1=True)

pred_time = time.time() - pred_start
print(f"  âœ“ äºˆæ¸¬å®Œäº†")
print(f"  â±ï¸  äºˆæ¸¬æ™‚é–“: {pred_time:.2f}ç§’")

# test_featuresã¯ã‚‚ã†ä¸è¦
del test_features
gc.collect()

# Submissionä½œæˆ
print("\n" + "=" * 80)
print("[STEP 7/7] ğŸ“ Submissionä½œæˆ")
print("=" * 80)
submission_start = time.time()

submission = sample_sub.copy()
submission["money_room"] = predictions.astype(int)

# predictionsã¯ä¸è¦
del predictions, sample_sub
gc.collect()

# ä¿å­˜
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = OUTPUT_DIR / f"submission_{timestamp}.csv"
submission.to_csv(output_path, index=False, header=False)

print(f"  âœ“ Submission saved: {output_path}")

# ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜
feature_importance = pd.DataFrame(
    {"feature": train_features.columns, "importance": models[0].feature_importances_}
).sort_values("importance", ascending=False)

importance_path = OUTPUT_DIR / f"feature_importance_{timestamp}.csv"
feature_importance.to_csv(importance_path, index=False)
print(f"  âœ“ Feature importance saved: {importance_path}")

submission_time = time.time() - submission_start
print(f"  â±ï¸  Submissionä½œæˆæ™‚é–“: {submission_time:.2f}ç§’")

# çµæœã‚µãƒãƒªãƒ¼
print("\n" + "=" * 80)
print("ğŸ‰ å®Œäº†!")
print("=" * 80)
print(f"ğŸ“Š CVçµæœ:")
print(f"  - MAPE: {np.mean(cv_scores):.4f}% (Â± {np.std(cv_scores):.4f}%)")
print(f"  - Fold scores: {[f'{s:.4f}%' for s in cv_scores]}")
print(f"\nğŸ“ˆ äºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
stats = submission["money_room"].describe()
print(f"  - Count: {int(stats['count']):,}")
print(f"  - Mean:  Â¥{int(stats['mean']):,}")
print(f"  - Std:   Â¥{int(stats['std']):,}")
print(f"  - Min:   Â¥{int(stats['min']):,}")
print(f"  - Max:   Â¥{int(stats['max']):,}")
print(f"\nğŸ“‚ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:")
print(f"  - Submission: {output_path.name}")
print(f"  - Feature importance: {importance_path.name}")
print(
    f"\nâ±ï¸  ç·å®Ÿè¡Œæ™‚é–“: {time.time() - overall_start_time:.2f}ç§’ ({(time.time() - overall_start_time)/60:.1f}åˆ†)"
)
print("\nTop 30 é‡è¦ãªç‰¹å¾´é‡:")
print(feature_importance.head(30).to_string(index=False))
print("\n" + "=" * 80)
print("âœ… Ready to submit! ğŸš€")
print("=" * 80)

# æœ€çµ‚çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
del train_features, models, feature_importance, submission
gc.collect()
