"""
åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³

æ–°è¦è¿½åŠ ç‰¹å¾´é‡:
1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ï¼ˆç·¯åº¦çµŒåº¦ï¼‰+ é›†ç´„ç‰¹å¾´é‡
2. Target Encodingï¼ˆåœ°åŸŸåˆ¥å¹³å‡ä¾¡æ ¼ãªã©ï¼‰
3. è·é›¢ç‰¹å¾´é‡ï¼ˆä¸»è¦éƒ½å¸‚ã¾ã§ã®è·é›¢ï¼‰
4. æ´¾ç”Ÿç‰¹å¾´é‡ï¼ˆç¯‰å¹´æ•°ã€å˜ä¾¡ãªã©ï¼‰
"""

import gc
import pickle
import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

import warnings  # noqa: E402

import numpy as np  # noqa: E402
import pandas as pd  # noqa: E402

from src.data.preprocess import preprocess_for_catboost  # noqa: E402
from src.features.geo_features import (  # noqa: E402
    create_cluster_aggregation_features,
    create_derived_features,
    create_distance_features,
    create_kmeans_clusters,
    create_target_encoding_features,
)
from src.models.train_catboost import predict_with_models, train_catboost_cv  # noqa: E402

warnings.filterwarnings("ignore")

# ãƒ‘ã‚¹è¨­å®š
DATA_DIR = project_root / "data"
TRAIN_PATH = DATA_DIR / "raw" / "train.csv"
TEST_PATH = DATA_DIR / "raw" / "test.csv"
SAMPLE_PATH = DATA_DIR / "raw" / "sample_submit.csv"
OUTPUT_DIR = project_root / "submissions" / "exp003_geo_features"
PROCESSED_DIR = DATA_DIR / "processed"

# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹
PROCESSED_TRAIN = PROCESSED_DIR / "train_processed.parquet"
PROCESSED_TEST = PROCESSED_DIR / "test_processed.parquet"
PROCESSED_TARGET = PROCESSED_DIR / "target.parquet"
PROCESSED_CAT_FEATURES = PROCESSED_DIR / "cat_features.pkl"

print("=" * 60)
print("åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³")
print("=" * 60)

# å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªã¨ãƒ­ãƒ¼ãƒ‰
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
use_cache = all(
    [
        PROCESSED_TRAIN.exists(),
        PROCESSED_TEST.exists(),
        PROCESSED_TARGET.exists(),
        PROCESSED_CAT_FEATURES.exists(),
    ]
)

if use_cache:
    print("\nâœ… å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã™...")
    train_features = pd.read_parquet(PROCESSED_TRAIN)
    test_features = pd.read_parquet(PROCESSED_TEST)
    target = pd.read_parquet(PROCESSED_TARGET).squeeze()
    with open(PROCESSED_CAT_FEATURES, "rb") as f:
        cat_features = pickle.load(f)

    print(f"Train shape: {train_features.shape}")
    print(f"Test shape: {test_features.shape}")
    print(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡æ•°: {len(cat_features)}")

else:
    print("\nğŸ”„ å‰å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™...")

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    print("\n[1] ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿...")
    train = pd.read_csv(TRAIN_PATH)
    test = pd.read_csv(TEST_PATH)

    print(f"Train shape: {train.shape}")
    print(f"Test shape: {test.shape}")

    # åŸºæœ¬å‰å‡¦ç†
    print("\n[2] åŸºæœ¬å‰å‡¦ç†...")
    train_features, test_features, target, cat_features = preprocess_for_catboost(
        train, test, target_col="money_room", apply_log=True
    )

    # å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰å‰Šé™¤
    del train, test
    gc.collect()

    # åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®è¿½åŠ 
    print("\n" + "=" * 60)
    print("åœ°ç†ç©ºé–“ç‰¹å¾´é‡ã®ä½œæˆ")
    print("=" * 60)

    # ç›®çš„å¤‰æ•°ã‚’ä¸€æ™‚çš„ã«çµåˆï¼ˆTarget Encodingç”¨ï¼‰
    train_with_target = train_features.copy()
    train_with_target["money_room"] = target

    # train_featuresã¯ä¸€æ—¦ä¸è¦
    del train_features
    gc.collect()

    # 1. K-meansã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
    train_with_target, test_features, kmeans = create_kmeans_clusters(
        train_with_target,
        test_features,
        lat_col="lat",
        lon_col="lon",
        n_clusters=50,
        random_state=42,
    )

    # kmeansãƒ¢ãƒ‡ãƒ«ã¯ä¸è¦
    del kmeans
    gc.collect()

    # 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã”ã¨ã®é›†ç´„ç‰¹å¾´é‡
    train_with_target, test_features = create_cluster_aggregation_features(
        train_with_target,
        test_features,
        target_col="money_room",
        cluster_col="geo_cluster",
        agg_cols=["house_area", "year_built", "walk_distance1", "money_kyoueki"],
    )

    # 3. Target Encoding
    train_with_target, test_features = create_target_encoding_features(
        train_with_target,
        test_features,
        target_col="money_room",
        categorical_cols=["city", "prefecture", "eki_name1"],
        smoothing=10.0,
    )

    # 4. è·é›¢ç‰¹å¾´é‡
    train_with_target = create_distance_features(
        train_with_target, lat_col="lat", lon_col="lon"
    )
    test_features = create_distance_features(
        test_features, lat_col="lat", lon_col="lon"
    )

    # 5. æ´¾ç”Ÿç‰¹å¾´é‡
    train_with_target = create_derived_features(train_with_target)
    test_features = create_derived_features(test_features)

    # ç›®çš„å¤‰æ•°ã‚’åˆ†é›¢
    target = train_with_target["money_room"]
    train_features = train_with_target.drop(columns=["money_room"])

    # train_with_targetã¯ä¸è¦
    del train_with_target
    gc.collect()

    print(f"\næœ€çµ‚çš„ãªç‰¹å¾´é‡æ•°: {len(train_features.columns)}")

    # ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡ã®æ›´æ–°ï¼ˆæ–°ã—ãè¿½åŠ ã•ã‚ŒãŸç‰¹å¾´é‡ã¯æ•°å€¤å‹ï¼‰
    # geo_clusterã¯ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ã¨ã—ã¦æ‰±ã†
    if "geo_cluster" in train_features.columns:
        train_features["geo_cluster"] = train_features["geo_cluster"].astype(str)
        test_features["geo_cluster"] = test_features["geo_cluster"].astype(str)
        if "geo_cluster" not in cat_features:
            cat_features.append("geo_cluster")

    print(f"ã‚«ãƒ†ã‚´ãƒªã‚«ãƒ«ç‰¹å¾´é‡æ•°: {len(cat_features)}")

    # å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
    print("\nğŸ’¾ å‰å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜...")
    train_features.to_parquet(PROCESSED_TRAIN, index=False)
    test_features.to_parquet(PROCESSED_TEST, index=False)
    pd.DataFrame({"target": target}).to_parquet(PROCESSED_TARGET, index=False)
    with open(PROCESSED_CAT_FEATURES, "wb") as f:
        pickle.dump(cat_features, f)
    print(f"ä¿å­˜å…ˆ: {PROCESSED_DIR}/")

# sample_submitã¯å¸¸ã«èª­ã¿è¾¼ã‚€ï¼ˆè»½ã„ã®ã§ï¼‰
sample_sub = pd.read_csv(SAMPLE_PATH, header=None, names=["id", "money_room"])

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
params = {
    "iterations": 500,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    "learning_rate": 0.05,
    "depth": 5,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    "loss_function": "MAE",
    "eval_metric": "MAE",
    "random_seed": 42,
    "verbose": 100,
    "early_stopping_rounds": 50,
}

# Cross Validation
print("\n" + "=" * 60)
print("Cross Validation")
print("=" * 60)
models, cv_scores = train_catboost_cv(
    train_features,
    target,
    cat_features,
    n_splits=3,
    params=params,
    verbose=100,  # ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
)

# targetã¯ã‚‚ã†ä¸è¦
del target
gc.collect()

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬
print("\n[3] ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬...")
predictions = predict_with_models(models, test_features, cat_features, apply_expm1=True)

# test_featuresã¯ã‚‚ã†ä¸è¦
del test_features
gc.collect()

# Submissionä½œæˆ
print("\n[4] Submissionä½œæˆ...")
submission = sample_sub.copy()
submission["money_room"] = predictions.astype(int)

# predictionsã¯ä¸è¦
del predictions, sample_sub
gc.collect()

# ä¿å­˜
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
output_path = OUTPUT_DIR / f"submission_{timestamp}.csv"
submission.to_csv(output_path, index=False, header=False)

print(f"\n{'='*60}")
print("å®Œäº†!")
print(f"{'='*60}")
print(f"Submission saved to: {output_path}")
print("\näºˆæ¸¬å€¤ã®çµ±è¨ˆ:")
print(submission["money_room"].describe())
print(f"\nCV MAPE: {np.mean(cv_scores):.4f}% (+/- {np.std(cv_scores):.4f}%)")
print(f"{'='*60}")
print("Ready to submit! ğŸš€")
print(f"{'='*60}")

# ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜
print("\n[5] ç‰¹å¾´é‡é‡è¦åº¦ã®ä¿å­˜...")
feature_importance = pd.DataFrame(
    {"feature": train_features.columns, "importance": models[0].feature_importances_}
).sort_values("importance", ascending=False)

importance_path = OUTPUT_DIR / f"feature_importance_{timestamp}.csv"
feature_importance.to_csv(importance_path, index=False)
print(f"Feature importance saved to: {importance_path}")

print("\nTop 30 é‡è¦ãªç‰¹å¾´é‡:")
print(feature_importance.head(30).to_string(index=False))

# æœ€çµ‚çš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
del train_features, models, feature_importance, submission
gc.collect()

print("\nâœ… ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
